{% extends 'guide_template.html' %}
{% block heading %}
  Model serving
{% endblock %}

{% block guide-content %}
<p>These tutorials are intended as quick examples to get started with working on ML model
  deployment and serving using Scilifelab Serve.</p>

<h4>Available resources</h4>
<ul>
<li>Tensorflow Serve</li>
<li>PyTorch Serve</li>
<li>Python Model Serve (Using FastApi)</li>
</ul>

<h4>Getting started</h4>
  
  <p>When a project is created, users also get a JupyterLab instance. This can be found in the
    Compute tab on the Sidebar (visible when you open a project). </p>
  <ol>
    <li> Click on <span style="font-weight: 500;color: #041c39c7;">'Open'</span> to go to the JupyterLab instance. </li>
    <li>On the home screen, click on <span style="font-weight: 500;color: #041c39c7;">'Terminal'</span> to open a terminal tab. </li> 
    <li>Clone the tutorials repository <a href="https://github.com/ScilifelabDataCentre/serve-tutorials">serve-tutorials</a>. 
      <ul>
        <li>Make sure you have git installed.</li>
          <li>Run the following command in the terminal:
            <code>$ git clone https://github.com/ScilifelabDataCentre/serve-tutorials</code></li>
      </ul>
    </li>
    <li> This creates a folder called <code>serve-tutorials</code> which contains necessary files for the remaining tutorial.</li>
    <li> Continue to either <a href="#tensorflow">Tensorflow serving</a>, <a href="#pytorch">PyTorch Serve.</a> or <a href="#python-deploy">Python Model Serve.</a></li>
  </ol>

  <h4 id="tensorflow">Tensorflow</h4>

  <ol start="6">
    <li>Go to <code>serve-tutorials/Tensorflow/AML</code> folder.</li>
    <li>Open the <code>aml.ipynb</code> notebook file that contains the code to create a ML model and save it as an object
      on Scilifelab Serve.</li>
    <li>Run all the cells in the notebook. This may take some time (around 20
      mins) as the model is trained.</li>
    <li>When complete, go to the Serve tab on the portal and create a Tensorflow Serve App
      with the follwing entries:
      <p><ul>
        <li><strong><em>Name</em>:</strong> AML Tensorflow</li>
        <li><strong><em>Subdomain</em>:</strong> This is automatically generated. A custom subdomain can be
          created by filling out the form below ("Add Subdomain"), this domain will then show up
          as an option.
        </li>
        <li><strong><em>Permissions</em>:</strong> Set the permission to project.</li>
        <!-- <li><strong>Persistent Volume:</strong> The volume that needs to be attached to the instance.</li> -->
        <li><strong><em>Model</em>:</strong> Select the model, in this case the model is named aml.</li>
        <li><strong><em>Flavor</em>:</strong> This indicates the resources given to the app. Currently on
          the Medium option is availble on the platform.
        </li>
      </ul></p>
      <p>This should create the app and it will be visible in the Serve Tab.</p>
    </li>
    <li>Copy the link for the service by right clicking <span style="font-weight: 500;color: #041c39c7;">Open</span> and selecting copy link address</li>
    <li>Go back to the JupyterLab instance and open the <code>predict.ipynb</code> notebook.</li>
    <li>Replace the endpoint with the URL that you copied and run the cells in the notebook. If you see a
      prediction, the service is running successfully.</li>
    
  </ol>

  <h4 id="pytorch">PyTorch Serve</h4>
  <p>To serve a PyTorch model, we will use a pytorch model archive (VGG11) already prepared. You
    can read more about the model <a href="https://serve-dev.scilifelab.se/https://arxiv.org/abs/1409.1556">here</a>.</p>
  
  <ol start="6">
    <li>Go to the <code>project-vol</code> folder inside the JupyterLab. Right-click to create a new
      folder called pytorch-deployment.</li>
    <li>Inside the terminal, change to the new directory by running <code>cd project-vol/pytorch-deployment/</code></li>
    <li>Run the command <code>curl -O -J https://nextcloud.dckube.scilifelab.se/s/TSNxFJCYrpJZAnz/download</code> to download the model archive</li>
    <li> Go to the Serve tab on the portal and create a PyTorch Serving App with the follwing entries:
      <p><ul>
        <li><strong><em>Name</em>:</strong> VGG11 Serve</li>
        <li><strong><em>Subdomain</em></strong> This is automatically generated, if not selected. You can
          create a subdomain to choose using the Add Subdomain form located at the bottom.
        </li>
        <li><strong><em>Permissions</em>:</strong> Set the permission to public. (Currently, public
          permissions are required to access the serve URL for pytorch apps).
        </li>
        <li><strong><em>Flavor</em>:</strong> This indicates the resources given to the app. Currently on
          the Medium option is availble on the platform.
        </li>
        <li><strong><em>Persistent Volume</em>:</strong> The volume that needs to be attached to the
          instance. In this case it is important to select project-vol as that is where we
          dowloaded the vgg11_scripted.mar file.
        </li>
        <li><strong><em>Path to Model Store</em>:</strong> This is the path to the folder where the model is
          located (relative to the mounted volume). As the model was downloaded in the
          pytorch-deployment folder inside the project-vol, the value for this will be
          pytorch-deployment.
        </li>
        <li><strong><em>Comma-separeted list of models (model1,model2,model3)</em>:</strong> This can be
          left empty. Here, model names from the model archive can be listed to deploy only those
          models.
        </li>
      </ul>
    </li><p>
    <p>This should create the app and it will be visible in the Serve Tab.</p>
    
    <li>Copy the link for the service by right clicking <span style="font-weight: 500;color: #041c39c7;">Open</span> and selecting copy link address</li>
    <li>Go to the terminal and change to <code>PyTorch</code> folder by running <code>cd serve-tutorials/PyTorch/</code> 
    <li> Run <code>curl &#60;copied-url&#62;/vgg11_scripted -T kitten.jpg</code> (Replace <code>&#60;copied-url&#62;</code> with the URL
    you copied earlier). If you should get a JSON response, the service is running successfully.
  </ol>

  <h4 id="python-deploy">Python Model Deployment</h4>
  <p>Launch a new terminal from within your JupyterLab instance and go to the Python-Model-Deployment
    folder:</p>
  <p><pre><code>cd serve-tutorials/Python-Model-Deployment folder</code></pre></p>
  <p>Install the required packages:</p>
  <p><code>pip install -r requirements.txt</code></p>
  <p>Go to <code>serve-tutorials/Python-Model-Deployment</code> folder. Open the
    <code> getting_started_with_swebert.ipynb</code>
    notebook file. Follow the notebook's instructions.</p>
  <p>Once you have run all the cells in the above notebook, open up again the terminal and execute
    the following commands within the repository directory (inside the Python-Model-Deployment
    folder):</p>
  <p><code>stackn create object afbert -r minor</code></p>
  <p><code>stackn get objects</code></p>
  <p>(Check that the model is listed; you should be able to see the newly created model object in
    your Scilifelab Serve UI, under the "<em>Objects</em>" tab)</p>

  <p>Deploy the newly created model object with the "<em>Python Model Deployment</em>" component
    (under the "<em>Serve</em>" tab). <em>Name</em> can be anything, <em>Model</em> should match
    the name of the newly created model (e.g., "afbert:v0.1.0"); leave the rest as defaults.</p>
  <p><strong>Note:</strong> It could take some time for this model to initialize, so keep
    checking the logs until it is available and wait until it is running successfully.</p>
  <p>Once the above serving app is up and running, copy the endpoint URL by right-clicking on the
    <em>Open</em> link.</p>
  <p>Go back to your JupyterLab and open the <code>predict.ipynb</code> notebook under the
    <em>notebooks</em> folder. Paste the copied URL in place of the URL in order to use the
    correct endpoint for the prediction.</p>
  <p>It is time to test the prediction! Run all the cells and check the results.</p>
  <p>You can play around by changing the values of the <code>example</code> and
    <code>msk_ind</code> variables. The latter will mask (or "hide") one of the words in the
    example sentence; then the prediction will shown the possible candidates for such "missing"
    word.</p>
{% endblock %}